{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***dataset***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.tensor([1,2,3,4],dtype=torch.float32)\n",
    "y=torch.tensor([2,4,6,8],dtype=torch.float32)\n",
    "\n",
    "w=torch.tensor(0.0,dtype=torch.float32,requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, there is a connection between the optimizer and the loss function in your code. Here's how it works:\n",
    "\n",
    "Loss Function (MSELoss): loss = nn.MSELoss() creates an object of the Mean Squared Error (MSE) loss function. This function calculates the difference between the predicted output (y_pred) and the actual target (y), computes their squared differences, and then takes the mean.\n",
    "\n",
    "Optimizer (SGD): optimizer = torch.optim.SGD([w], lr=learning_rate) creates an optimizer object using stochastic gradient descent (SGD). It optimizes the model parameters ([w] in this case) based on the gradients of the loss function with respect to these parameters.\n",
    "\n",
    "Gradient Calculation and Parameter Update: In your training loop, you calculate the loss using l = loss_fn(y, y_pred) and then call l.backward() to compute the gradients of the loss function with respect to the model parameters (w in this case). The optimizer is then used to update the weights (w) based on these gradients and the specified learning rate using optimizer.step().\n",
    "\n",
    "So, the optimizer uses the gradients computed from the loss function to update the model parameters (weights) in a way that minimizes the loss, which is the fundamental principle of training a neural network through optimization algorithms like SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=0.01\n",
    "n_iters=100\n",
    "loss=nn.MSELoss()\n",
    "optimizer=torch.optim.SGD([w],lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x):\n",
    "    return w*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no: 0 ,loss= 30.0,w=0.3\n",
      "epoch no: 2 ,loss= 15.7,w=0.772\n",
      "epoch no: 4 ,loss= 8.17,w=1.11\n",
      "epoch no: 6 ,loss= 4.27,w=1.36\n",
      "epoch no: 8 ,loss= 2.23,w=1.54\n",
      "epoch no: 10 ,loss= 1.16,w=1.67\n",
      "epoch no: 12 ,loss= 0.607,w=1.76\n",
      "epoch no: 14 ,loss= 0.317,w=1.83\n",
      "epoch no: 16 ,loss= 0.165,w=1.87\n",
      "epoch no: 18 ,loss= 0.0863,w=1.91\n",
      "epoch no: 20 ,loss= 0.0451,w=1.93\n",
      "epoch no: 22 ,loss= 0.0235,w=1.95\n",
      "epoch no: 24 ,loss= 0.0123,w=1.97\n",
      "epoch no: 26 ,loss= 0.00641,w=1.98\n",
      "epoch no: 28 ,loss= 0.00335,w=1.98\n",
      "epoch no: 30 ,loss= 0.00175,w=1.99\n",
      "epoch no: 32 ,loss= 0.000912,w=1.99\n",
      "epoch no: 34 ,loss= 0.000476,w=1.99\n",
      "epoch no: 36 ,loss= 0.000248,w=2.0\n",
      "epoch no: 38 ,loss= 0.00013,w=2.0\n",
      "epoch no: 40 ,loss= 6.77e-05,w=2.0\n",
      "epoch no: 42 ,loss= 3.53e-05,w=2.0\n",
      "epoch no: 44 ,loss= 1.84e-05,w=2.0\n",
      "epoch no: 46 ,loss= 9.63e-06,w=2.0\n",
      "epoch no: 48 ,loss= 5.03e-06,w=2.0\n",
      "epoch no: 50 ,loss= 2.62e-06,w=2.0\n",
      "epoch no: 52 ,loss= 1.37e-06,w=2.0\n",
      "epoch no: 54 ,loss= 7.15e-07,w=2.0\n",
      "epoch no: 56 ,loss= 3.74e-07,w=2.0\n",
      "epoch no: 58 ,loss= 1.95e-07,w=2.0\n",
      "epoch no: 60 ,loss= 1.02e-07,w=2.0\n",
      "epoch no: 62 ,loss= 5.32e-08,w=2.0\n",
      "epoch no: 64 ,loss= 2.77e-08,w=2.0\n",
      "epoch no: 66 ,loss= 1.45e-08,w=2.0\n",
      "epoch no: 68 ,loss= 7.59e-09,w=2.0\n",
      "epoch no: 70 ,loss= 3.97e-09,w=2.0\n",
      "epoch no: 72 ,loss= 2.06e-09,w=2.0\n",
      "epoch no: 74 ,loss= 1.07e-09,w=2.0\n",
      "epoch no: 76 ,loss= 5.53e-10,w=2.0\n",
      "epoch no: 78 ,loss= 2.88e-10,w=2.0\n",
      "epoch no: 80 ,loss= 1.47e-10,w=2.0\n",
      "epoch no: 82 ,loss= 7.32e-11,w=2.0\n",
      "epoch no: 84 ,loss= 3.81e-11,w=2.0\n",
      "epoch no: 86 ,loss= 2.03e-11,w=2.0\n",
      "epoch no: 88 ,loss= 1.11e-11,w=2.0\n",
      "epoch no: 90 ,loss= 5.08e-12,w=2.0\n",
      "epoch no: 92 ,loss= 2.77e-12,w=2.0\n",
      "epoch no: 94 ,loss= 8.99e-13,w=2.0\n",
      "epoch no: 96 ,loss= 8.99e-13,w=2.0\n",
      "epoch no: 98 ,loss= 8.99e-13,w=2.0\n",
      "tensor([2.0000, 4.0000, 6.0000, 8.0000], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_iters):\n",
    "    y_pred=forward(x)\n",
    "\n",
    "    l=loss(y,y_pred)\n",
    "\n",
    "    # gradient calculation:\n",
    "    l.backward()\n",
    "\n",
    "    # with torch.no_grad():\n",
    "    #     w-= learning_rate * w.grad\n",
    "    # update weights\n",
    "    optimizer.step()\n",
    "\n",
    "    # Zero the gradient:\n",
    "    # w.grad.zero_()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if(epoch%2==0):\n",
    "        print(f\"epoch no: {epoch} ,loss= {l:.3},w={w:.3}\")\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
